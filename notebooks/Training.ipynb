{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHa9hMUHlMMN"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/batu-el/understanding-inductive-biases-of-gnns/blob/main/notebooks/Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "tVe55J-aw7aC"
      },
      "outputs": [],
      "source": [
        "# Combining Attention values across heads - Avg\n",
        "# Combining Attention values across layers - Matrix Multiply"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRR5bUiLRZ5a"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLx8ARPERaaB",
        "outputId": "608d67fe-393e-4268-a3ac-143b0283e05a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dgl in /usr/local/lib/python3.11/dist-packages (2.1.0)\n",
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (1.14.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.11/dist-packages (from dgl) (3.4.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from dgl) (4.67.1)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: torchdata>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (0.11.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.14)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl) (2025.1.31)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Installing PyTorch Geometric\n",
            "Installing other libraries\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.4.2)\n",
            "Requirement already satisfied: lovely-tensors in /usr/local/lib/python3.11/dist-packages (0.1.18)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from lovely-tensors) (2.5.1+cu121)\n",
            "Requirement already satisfied: lovely-numpy>=0.2.13 in /usr/local/lib/python3.11/dist-packages (from lovely-tensors) (0.2.13)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from lovely-numpy>=0.2.13->lovely-tensors) (2.0.2)\n",
            "Requirement already satisfied: fastcore in /usr/local/lib/python3.11/dist-packages (from lovely-numpy>=0.2.13->lovely-tensors) (1.7.29)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (from lovely-numpy>=0.2.13->lovely-tensors) (7.34.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from lovely-numpy>=0.2.13->lovely-tensors) (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->lovely-tensors) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->lovely-tensors) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->lovely-tensors) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->lovely-tensors) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->lovely-tensors) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->lovely-tensors) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->lovely-tensors) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->lovely-tensors) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->lovely-tensors) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch->lovely-tensors) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch->lovely-tensors) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch->lovely-tensors) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch->lovely-tensors) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch->lovely-tensors) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->lovely-tensors) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->lovely-tensors) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->lovely-tensors) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->lovely-tensors) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->lovely-tensors) (12.4.127)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->lovely-tensors) (1.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from fastcore->lovely-numpy>=0.2.13->lovely-tensors) (24.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython->lovely-numpy>=0.2.13->lovely-tensors) (75.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython->lovely-numpy>=0.2.13->lovely-tensors) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython->lovely-numpy>=0.2.13->lovely-tensors) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython->lovely-numpy>=0.2.13->lovely-tensors) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython->lovely-numpy>=0.2.13->lovely-tensors) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython->lovely-numpy>=0.2.13->lovely-tensors) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython->lovely-numpy>=0.2.13->lovely-tensors) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython->lovely-numpy>=0.2.13->lovely-tensors) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython->lovely-numpy>=0.2.13->lovely-tensors) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython->lovely-numpy>=0.2.13->lovely-tensors) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->lovely-tensors) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lovely-numpy>=0.2.13->lovely-tensors) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lovely-numpy>=0.2.13->lovely-tensors) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lovely-numpy>=0.2.13->lovely-tensors) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lovely-numpy>=0.2.13->lovely-tensors) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lovely-numpy>=0.2.13->lovely-tensors) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lovely-numpy>=0.2.13->lovely-tensors) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lovely-numpy>=0.2.13->lovely-tensors) (2.8.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython->lovely-numpy>=0.2.13->lovely-tensors) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython->lovely-numpy>=0.2.13->lovely-tensors) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->lovely-numpy>=0.2.13->lovely-tensors) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->lovely-numpy>=0.2.13->lovely-tensors) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install dgl torch_geometric torch\n",
        "\n",
        "# Install required python libraries\n",
        "import os\n",
        "\n",
        "# Install PyTorch Geometric and other libraries\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "    print(\"Installing PyTorch Geometric\")\n",
        "    !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n",
        "    !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n",
        "    !pip install -q torch-geometric\n",
        "    print(\"Installing other libraries\")\n",
        "    !pip install networkx\n",
        "    !pip install lovely-tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMiwOILkRgEP",
        "outputId": "b9b51892-3389-4b09-8284-8acd8968cc5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All imports succeeded.\n",
            "Python version 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n",
            "PyTorch version 2.5.1+cu121\n",
            "PyG version 2.6.1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import itertools\n",
        "from datetime import datetime\n",
        "from typing import Mapping, Tuple, Sequence, List\n",
        "\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Embedding, Linear, ReLU, BatchNorm1d, LayerNorm, Module, ModuleList, Sequential\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer, MultiheadAttention\n",
        "from torch.optim import Adam\n",
        "\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data, Batch\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.datasets import Planetoid\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.utils import remove_self_loops, dense_to_sparse, to_dense_batch, to_dense_adj\n",
        "\n",
        "from torch_geometric.nn import GCNConv, GATConv, GATv2Conv\n",
        "\n",
        "# from torch_scatter import scatter, scatter_mean, scatter_max, scatter_sum\n",
        "\n",
        "import lovely_tensors as lt\n",
        "lt.monkey_patch()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "# warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "# warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "print(\"All imports succeeded.\")\n",
        "print(\"Python version {}\".format(sys.version))\n",
        "print(\"PyTorch version {}\".format(torch.__version__))\n",
        "print(\"PyG version {}\".format(torch_geometric.__version__))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuVTx4otRi6i",
        "outputId": "66187d7b-b8b8-4d09-9fbf-d39fda8462b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All seeds set.\n"
          ]
        }
      ],
      "source": [
        "# Set random seed for deterministic results\n",
        "\n",
        "def seed(seed=0):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed(0)\n",
        "print(\"All seeds set.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6R5GR7hRdEI"
      },
      "source": [
        "# Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "8PVfelu6ReJe"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.datasets import WebKB, WikipediaNetwork\n",
        "\n",
        "DATASETS = {}\n",
        "\n",
        "# Chamelion & Squirrel\n",
        "# Cora & Citeseer\n",
        "# Cornell & Texas & Wisconsin\n",
        "\n",
        "## Mid Size Datasets\n",
        "# Citation Networks\n",
        "dataset = 'Cora'\n",
        "dataset = Planetoid('/tmp/Cora', dataset)\n",
        "data = dataset[0]\n",
        "DATASETS['Cora'] = data\n",
        "dataset = 'Citeseer'\n",
        "dataset = Planetoid('/tmp/Citeseer', dataset)\n",
        "data = dataset[0]\n",
        "DATASETS['Citeseer'] = data\n",
        "# Wikipedia Pages\n",
        "dataset = 'Chameleon'\n",
        "dataset = WikipediaNetwork(root='/tmp/Chameleon', name='Chameleon')\n",
        "data = dataset[0]\n",
        "DATASETS['Chameleon'] = data\n",
        "dataset = 'Squirrel'\n",
        "dataset = WikipediaNetwork(root='/tmp/Squirrel', name='Squirrel')\n",
        "data = dataset[0]\n",
        "DATASETS['Squirrel'] = data\n",
        "### Small Sized Datasets\n",
        "# Web Pages\n",
        "dataset = WebKB(root='/tmp/Cornell', name='Cornell')\n",
        "data = dataset[0]\n",
        "DATASETS['Cornell'] = data\n",
        "dataset = WebKB(root='/tmp/Texas', name='Texas')\n",
        "data = dataset[0]\n",
        "DATASETS['Texas'] = data\n",
        "dataset = WebKB(root='/tmp/Wisconsin', name='Wisconsin')\n",
        "data = dataset[0]\n",
        "DATASETS['Wisconsin'] = data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "PqcrQ7FuVa8c"
      },
      "outputs": [],
      "source": [
        "# import tqdm\n",
        "# ### Shortest Paths ###\n",
        "# def get_shortest_path_matrix(adjacency_matrix):\n",
        "#     graph = nx.from_numpy_array(adjacency_matrix.cpu().numpy(), create_using=nx.DiGraph)\n",
        "#     shortest_path_matrix = nx.floyd_warshall_numpy(graph)\n",
        "#     shortest_path_matrix = torch.tensor(shortest_path_matrix).float()\n",
        "#     return shortest_path_matrix\n",
        "\n",
        "# SHORTEST_PATHS = {}\n",
        "# for data_key in tqdm.tqdm(DATASETS):\n",
        "#   print(data_key)\n",
        "#   data = DATASETS[data_key]\n",
        "#   dense_adj = to_dense_adj(data.edge_index, max_num_nodes = data.x.shape[0])[0]\n",
        "#   dense_shortest_path_matrix = get_shortest_path_matrix(dense_adj)\n",
        "#   SHORTEST_PATHS[data_key] = dense_shortest_path_matrix\n",
        "\n",
        "# ### Save the Shortest Paths\n",
        "# import pickle\n",
        "# with open('sp_dict.pkl', 'wb') as f:\n",
        "#     pickle.dump(SHORTEST_PATHS, f)\n",
        "import pickle\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/sp_dict2.pkl', 'rb') as f:\n",
        "    SHORTEST_PATHS = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "hzEswGTMXoOI"
      },
      "outputs": [],
      "source": [
        "for data_key in DATASETS:\n",
        "  data = DATASETS[data_key]\n",
        "  data.dense_sp_matrix = SHORTEST_PATHS[data_key]\n",
        "  data.dense_adj = to_dense_adj(data.edge_index, max_num_nodes = data.x.shape[0])[0]\n",
        "  data.dense_adj = data.dense_adj.cuda() + torch.eye(data.dense_adj.shape[0]).cuda()\n",
        "  data.dense_adj[data.dense_adj == 2] = 1\n",
        "  data = T.AddLaplacianEigenvectorPE(k = 16, attr_name = 'pos_enc')(data)\n",
        "  DATASETS[data_key] = data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "vsqv_LTEU-2H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "077e26bc-e3de-4287-9b4b-5284a13f0c14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have 10 Masks\n",
            "Train Ratio: 0.39992615580558777\n",
            "Val Ratio: 0.29985228180885315\n",
            "Test Ratio: 0.3002215623855591\n",
            "We have 10 Masks\n",
            "Train Ratio: 0.39975953102111816\n",
            "Val Ratio: 0.29996994137763977\n",
            "Test Ratio: 0.30027052760124207\n",
            "We have 10 Masks\n",
            "Train Ratio: 0.39964866638183594\n",
            "Val Ratio: 0.2999560832977295\n",
            "Test Ratio: 0.30039525032043457\n",
            "We have 10 Masks\n",
            "Train Ratio: 0.39992308616638184\n",
            "Val Ratio: 0.2999423146247864\n",
            "Test Ratio: 0.3001345992088318\n",
            "We have 10 Masks\n",
            "Train Ratio: 0.3989070951938629\n",
            "Val Ratio: 0.2950819730758667\n",
            "Test Ratio: 0.3060109317302704\n",
            "We have 10 Masks\n",
            "Train Ratio: 0.3989070951938629\n",
            "Val Ratio: 0.2950819730758667\n",
            "Test Ratio: 0.3060109317302704\n",
            "We have 10 Masks\n",
            "Train Ratio: 0.39840638637542725\n",
            "Val Ratio: 0.29880478978157043\n",
            "Test Ratio: 0.3027888536453247\n"
          ]
        }
      ],
      "source": [
        "### Masks ###\n",
        "\n",
        "def generate_masks(num_nodes=None,num_runs=None,train_ratio=None, val_ratio=None):\n",
        "    masks = { 'train_mask': np.zeros((num_nodes, num_runs), dtype=int),\n",
        "              'val_mask': np.zeros((num_nodes, num_runs), dtype=int),\n",
        "              'test_mask': np.zeros((num_nodes, num_runs), dtype=int)}\n",
        "\n",
        "    for run in range(num_runs):\n",
        "        indices = np.arange(num_nodes)\n",
        "        np.random.shuffle(indices)\n",
        "        train_end = int(train_ratio * num_nodes)\n",
        "        val_end = train_end + int(val_ratio * num_nodes)\n",
        "        masks['train_mask'][indices[:train_end], run] = 1\n",
        "        masks['val_mask'][indices[train_end:val_end], run] = 1\n",
        "        masks['test_mask'][indices[val_end:], run] = 1\n",
        "\n",
        "    tensor_masks = {'train_mask': torch.tensor(masks['train_mask']),\n",
        "                    'val_mask':torch.tensor(masks['val_mask']),\n",
        "                    'test_mask':torch.tensor(masks['test_mask'])}\n",
        "    return tensor_masks\n",
        "\n",
        "for data_key in DATASETS:\n",
        "    data = DATASETS[data_key]\n",
        "\n",
        "    masks = generate_masks(num_nodes=data.x.shape[0], num_runs=10, train_ratio=0.4, val_ratio=0.3)\n",
        "    data.train_mask = masks['train_mask'].bool()\n",
        "    data.val_mask = masks['val_mask'].bool()\n",
        "    data.test_mask = masks['test_mask'].bool()\n",
        "\n",
        "    if len(data.train_mask.shape)==1:\n",
        "      print('Add 10 Masks')\n",
        "    else:\n",
        "      print('We have 10 Masks')\n",
        "      print('Train Ratio:',(data.train_mask[:,0].sum() / len(data.train_mask[:,0])).item())\n",
        "      print('Val Ratio:',(data.val_mask[:,0].sum() / len(data.val_mask[:,0])).item())\n",
        "      print('Test Ratio:',(data.test_mask[:,0].sum() / len(data.test_mask[:,0])).item())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install dgl==2.5\n"
      ],
      "metadata": {
        "id": "aeeVpIr0py9p",
        "outputId": "aec3c390-7a6e-447e-ea6d-87b4be1aad75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement dgl==2.5 (from versions: 0.1.0, 0.1.2, 0.1.3, 1.0.0, 1.0.1, 1.0.4, 1.1.0, 1.1.1, 1.1.2, 1.1.3, 2.1.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for dgl==2.5\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPGL9tFORJCQ"
      },
      "source": [
        "## Table 1: Dataset Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "O34_wDE-RLre",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "4cd9e6f2-b384-4c89-c15e-5a6c18205181"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'dgl' has no attribute 'node_homophily'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-c703e4b74b45>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   Homophily_Levels[data_key] = {'Node Homophily':dgl.node_homophily(g, g.ndata['y'])*100,\n\u001b[0m\u001b[1;32m     12\u001b[0m                                 \u001b[0;34m'Edge Homophily'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_homophily\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                 \u001b[0;34m'Adjusted Homophily'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjusted_homophily\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'dgl' has no attribute 'node_homophily'"
          ]
        }
      ],
      "source": [
        "### Table 1 ###\n",
        "### Dataset Statistics ###\n",
        "import dgl\n",
        "Homophily_Levels = {}\n",
        "\n",
        "for data_key in DATASETS:\n",
        "  data = DATASETS[data_key]\n",
        "  edge_index_tensor = torch.tensor(data.edge_index.cpu().numpy(), dtype=torch.long)\n",
        "  g = dgl.graph((edge_index_tensor[0], edge_index_tensor[1]), num_nodes=data.x.shape[0])\n",
        "  g.ndata['y'] = torch.tensor(data.y.cpu().numpy(), dtype=torch.long)\n",
        "  Homophily_Levels[data_key] = {'Node Homophily':dgl.node_homophily(g, g.ndata['y'])*100,\n",
        "                                'Edge Homophily':dgl.edge_homophily(g, g.ndata['y'])*100,\n",
        "                                'Adjusted Homophily':dgl.adjusted_homophily(g, g.ndata['y'])*100,\n",
        "                                'Number of Nodes': int(g.num_nodes()),\n",
        "                                'Number of Edges': int(g.num_edges())\n",
        "                                }\n",
        "df = pd.DataFrame(Homophily_Levels).round(1)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "print(dgl.__version__)\n",
        "\n",
        "import torch\n",
        "print(torch.version.cuda)\n",
        "\n",
        "print(torch.__version__)\n",
        "\n"
      ],
      "metadata": {
        "id": "x-WFQ5jtoalg",
        "outputId": "1c15c497-71fe-4c3c-f941-7c2aa5b93639",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu121\n",
            "12.1\n",
            "2.5.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JV4ZtidSENR"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKL9b7tCSDDd"
      },
      "outputs": [],
      "source": [
        "# PyG example code: https://github.com/pyg-team/pytorch_geometric/blob/master/examples/gcn2_cora.py\n",
        "\n",
        "class GNNModel(Module):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_dim: int = data.x.shape[-1],\n",
        "            hidden_dim: int = 128,\n",
        "            num_heads: int = 1,\n",
        "            num_layers: int = 1,\n",
        "            out_dim: int = len(data.y.unique()),\n",
        "            dropout: float = 0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lin_in = Linear(in_dim, hidden_dim)\n",
        "        self.lin_out = Linear(hidden_dim, out_dim)\n",
        "        self.layers = ModuleList()\n",
        "\n",
        "        for layer in range(num_layers):\n",
        "            self.layers.append(\n",
        "                GCNConv(hidden_dim, hidden_dim)\n",
        "            )\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "\n",
        "        x = self.lin_in(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            # conv -> activation ->  dropout -> residual\n",
        "            x_in = x\n",
        "            x = layer(x, edge_index)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, self.dropout, training=self.training)\n",
        "            x = x_in + x\n",
        "\n",
        "        x = self.lin_out(x)\n",
        "        return x.log_softmax(dim=-1)\n",
        "\n",
        "\n",
        "class SparseGraphTransformerModel(Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_dim: int = data.x.shape[-1],\n",
        "            hidden_dim: int = 128,\n",
        "            num_heads: int = 1,\n",
        "            num_layers: int = 1,\n",
        "            out_dim: int = len(data.y.unique()),\n",
        "            dropout: float = 0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lin_in = Linear(in_dim, hidden_dim)\n",
        "        self.lin_out = Linear(hidden_dim, out_dim)\n",
        "\n",
        "        self.layers = ModuleList()\n",
        "        for layer in range(num_layers):\n",
        "            self.layers.append(\n",
        "                MultiheadAttention(\n",
        "                    embed_dim = hidden_dim,\n",
        "                    num_heads = num_heads,\n",
        "                    dropout = dropout\n",
        "                )\n",
        "            )\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, dense_adj):\n",
        "\n",
        "        x = self.lin_in(x)\n",
        "\n",
        "        self.attn_weights_list = []\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x_in = x\n",
        "            x, attn_weights = layer(\n",
        "                x, x, x,\n",
        "                attn_mask = ~dense_adj.bool(),\n",
        "                average_attn_weights = False\n",
        "            )\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, self.dropout, training=self.training)\n",
        "            x = x_in + x\n",
        "\n",
        "            self.attn_weights_list.append(attn_weights)\n",
        "\n",
        "        x = self.lin_out(x)\n",
        "\n",
        "        return x.log_softmax(dim=-1)\n",
        "\n",
        "class DenseGraphTransformerModel(Module):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_dim: int = data.x.shape[-1],\n",
        "            pos_enc_dim: int = 16,\n",
        "            hidden_dim: int = 128,\n",
        "            num_heads: int = 1,\n",
        "            num_layers: int = 1,\n",
        "            out_dim: int = len(data.y.unique()),\n",
        "            dropout: float = 0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lin_in = Linear(in_dim, hidden_dim)\n",
        "        self.lin_pos_enc = Linear(pos_enc_dim, hidden_dim)\n",
        "        self.lin_out = Linear(hidden_dim, out_dim)\n",
        "\n",
        "        self.layers = ModuleList()\n",
        "        for layer in range(num_layers):\n",
        "            self.layers.append(\n",
        "                MultiheadAttention(\n",
        "                    embed_dim = hidden_dim,\n",
        "                    num_heads = num_heads,\n",
        "                    dropout = dropout\n",
        "                )\n",
        "            )\n",
        "\n",
        "\n",
        "        self.attn_bias_scale = torch.nn.Parameter(torch.tensor([10.0]))  # controls how much we initially bias our model to nearby nodes\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, pos_enc, dense_sp_matrix):\n",
        "\n",
        "        # x = self.lin_in(x) + self.lin_pos_enc(pos_enc)\n",
        "        x = self.lin_in(x)  # no node positional encoding\n",
        "\n",
        "        # attention bias\n",
        "        # [i, j] -> inverse of shortest path distance b/w node i and j\n",
        "        # diagonals -> self connection, set to 0\n",
        "        # disconnected nodes -> -1\n",
        "        attn_bias = self.attn_bias_scale * torch.nan_to_num(\n",
        "            (1 / (torch.nan_to_num(dense_sp_matrix, nan=-1, posinf=-1, neginf=-1))),\n",
        "            nan=0, posinf=0, neginf=0)\n",
        "        #attn_bias = torch.ones_like(attn_bias)\n",
        "\n",
        "        # TransformerEncoder\n",
        "        # x = self.encoder(x, mask = attn_bias)\n",
        "\n",
        "        self.attn_weights_list = []\n",
        "\n",
        "        for layer in self.layers:\n",
        "            # MHSA layer\n",
        "            # float mask adds learnable additive attention bias\n",
        "            x_in = x\n",
        "            x, attn_weights = layer(\n",
        "                x, x, x,\n",
        "                attn_mask = attn_bias,\n",
        "                average_attn_weights = False\n",
        "            )\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, self.dropout, training=self.training)\n",
        "            x = x_in + x\n",
        "\n",
        "            self.attn_weights_list.append(attn_weights)\n",
        "\n",
        "        x = self.lin_out(x)\n",
        "\n",
        "        return x.log_softmax(dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "class DenseGraphTransformerModel_V2(Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_dim: int = data.x.shape[-1],\n",
        "            pos_enc_dim: int = 16,\n",
        "            hidden_dim: int = 128,\n",
        "            num_heads: int = 1,\n",
        "            num_layers: int = 1,\n",
        "            out_dim: int = len(data.y.unique()),\n",
        "            dropout: float = 0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lin_in = Linear(in_dim, hidden_dim)\n",
        "        self.lin_pos_enc = Linear(pos_enc_dim, hidden_dim)\n",
        "        self.lin_out = Linear(hidden_dim, out_dim)\n",
        "\n",
        "        self.layers = ModuleList()\n",
        "        for layer in range(num_layers):\n",
        "            self.layers.append(\n",
        "                MultiheadAttention(\n",
        "                    embed_dim = hidden_dim,\n",
        "                    num_heads = num_heads,\n",
        "                    dropout = dropout\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.attn_bias_scale = torch.nn.Parameter(torch.tensor([10.0]))  # controls how much we initially bias our model to nearby nodes\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, pos_enc, dense_sp_matrix):\n",
        "\n",
        "        x = self.lin_in(x) + self.lin_pos_enc(pos_enc)\n",
        "        # x = self.lin_in(x)  # no node positional encoding\n",
        "\n",
        "        # attention bias\n",
        "        # [i, j] -> inverse of shortest path distance b/w node i and j\n",
        "        # diagonals -> self connection, set to 0\n",
        "        # disconnected nodes -> -1\n",
        "        # attn_bias = self.attn_bias_scale * torch.nan_to_num(\n",
        "        #     (1 / (torch.nan_to_num(dense_sp_matrix, nan=-1, posinf=-1, neginf=-1))),\n",
        "        #     nan=0, posinf=0, neginf=0\n",
        "        # )\n",
        "        #attn_bias = torch.ones_like(attn_bias)\n",
        "\n",
        "        # TransformerEncoder\n",
        "        # x = self.encoder(x, mask = attn_bias)\n",
        "\n",
        "        self.attn_weights_list = []\n",
        "\n",
        "        for layer in self.layers:\n",
        "            # # TransformerEncoderLayer\n",
        "            # # float mask adds learnable additive attention bias\n",
        "            # x = layer(x, src_mask = attn_bias)\n",
        "\n",
        "            # MHSA layer\n",
        "            # float mask adds learnable additive attention bias\n",
        "            x_in = x\n",
        "            x, attn_weights = layer(\n",
        "                x, x, x,\n",
        "                # attn_mask = attn_bias,\n",
        "                average_attn_weights = False\n",
        "            )\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, self.dropout, training=self.training)\n",
        "            x = x_in + x\n",
        "\n",
        "            self.attn_weights_list.append(attn_weights)\n",
        "\n",
        "        x = self.lin_out(x)\n",
        "\n",
        "        return x.log_softmax(dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z39ODu9oT2f9"
      },
      "source": [
        "# Trainers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgXWrDZ5V_al"
      },
      "outputs": [],
      "source": [
        "def Train_GCN(NUM_LAYERS,\n",
        "              NUM_HEADS,\n",
        "              data):\n",
        "\n",
        "    IN_DIM = data.x.shape[-1]\n",
        "    OUT_DIM = len(data.y.unique())\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    model = GNNModel(num_layers=NUM_LAYERS, num_heads=NUM_HEADS, in_dim=IN_DIM,out_dim=OUT_DIM).to(device)\n",
        "    data = data.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "    def train():\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index)\n",
        "        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        return float(loss)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def test():\n",
        "        model.eval()\n",
        "        pred, accs = model(data.x, data.edge_index).argmax(dim=-1), []\n",
        "        for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "            accs.append(int((pred[mask] == data.y[mask]).sum()) / int(mask.sum()))\n",
        "        return accs\n",
        "\n",
        "    best_val_acc = test_acc = 0\n",
        "    times = []\n",
        "    for epoch in range(1, 100):\n",
        "        start = time.time()\n",
        "        loss = train()\n",
        "        train_acc, val_acc, tmp_test_acc = test()\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            test_acc = tmp_test_acc\n",
        "        # print(f'Epoch: {epoch:04d}, Loss: {loss:.4f} Train: {train_acc:.4f}, '\n",
        "        #       f'Val: {val_acc:.4f}, Test: {tmp_test_acc:.4f}, '\n",
        "        #       f'Final Test: {test_acc:.4f}')\n",
        "        times.append(time.time() - start)\n",
        "    # print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")\n",
        "    return {'train_acc':train_acc,'val_acc':val_acc,'test_acc':test_acc}, None\n",
        "\n",
        "def Train_SparseGraphTransformerModel(NUM_LAYERS,\n",
        "              NUM_HEADS,\n",
        "              data):\n",
        "\n",
        "    IN_DIM = data.x.shape[-1]\n",
        "    OUT_DIM = len(data.y.unique())\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    model = SparseGraphTransformerModel(num_layers=NUM_LAYERS, num_heads=NUM_HEADS, in_dim=IN_DIM,out_dim=OUT_DIM).to(device)\n",
        "    data = data.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    def train():\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.dense_adj)\n",
        "        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        return float(loss)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def test():\n",
        "        model.eval()\n",
        "        pred, accs = model(data.x, data.dense_adj).argmax(dim=-1), []\n",
        "        for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "            accs.append(int((pred[mask] == data.y[mask]).sum()) / int(mask.sum()))\n",
        "        return accs\n",
        "\n",
        "    best_val_acc = test_acc = 0\n",
        "    times = []\n",
        "    for epoch in range(1, 100):\n",
        "        start = time.time()\n",
        "        loss = train()\n",
        "        train_acc, val_acc, tmp_test_acc = test()\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            test_acc = tmp_test_acc\n",
        "        # print(f'Epoch: {epoch:04d}, Loss: {loss:.4f} Train: {train_acc:.4f}, '\n",
        "        #       f'Val: {val_acc:.4f}, Test: {tmp_test_acc:.4f}, '\n",
        "        #       f'Final Test: {test_acc:.4f}')\n",
        "        times.append(time.time() - start)\n",
        "    # print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")\n",
        "    return {'train_acc':train_acc,'val_acc':val_acc,'test_acc':test_acc}, model.attn_weights_list\n",
        "\n",
        "def Train_DenseGraphTransformerModel(NUM_LAYERS,\n",
        "              NUM_HEADS,\n",
        "              data):\n",
        "\n",
        "    IN_DIM = data.x.shape[-1]\n",
        "    OUT_DIM = len(data.y.unique())\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    model = DenseGraphTransformerModel(num_layers=NUM_LAYERS, num_heads=NUM_HEADS, in_dim=IN_DIM,out_dim=OUT_DIM).to(device)\n",
        "    data = data.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    def train():\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        # print(data.pos_enc)\n",
        "        out = model(data.x, data.pos_enc, data.dense_sp_matrix)\n",
        "        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        return float(loss)\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def test():\n",
        "        model.eval()\n",
        "        pred, accs = model(data.x, data.pos_enc, data.dense_sp_matrix).argmax(dim=-1), []\n",
        "        for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "            accs.append(int((pred[mask] == data.y[mask]).sum()) / int(mask.sum()))\n",
        "        return accs\n",
        "\n",
        "    best_val_acc = test_acc = 0\n",
        "    times = []\n",
        "    for epoch in range(1, 100):\n",
        "        start = time.time()\n",
        "        loss = train()\n",
        "        train_acc, val_acc, tmp_test_acc = test()\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            test_acc = tmp_test_acc\n",
        "        # print(f'Epoch: {epoch:04d}, Loss: {loss:.4f} Train: {train_acc:.4f}, '\n",
        "        #       f'Val: {val_acc:.4f}, Test: {tmp_test_acc:.4f}, '\n",
        "        #       f'Final Test: {test_acc:.4f}')\n",
        "        times.append(time.time() - start)\n",
        "    # print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")\n",
        "    return {'train_acc':train_acc,'val_acc':val_acc,'test_acc':test_acc}, model.attn_weights_list\n",
        "\n",
        "    # Notes\n",
        "    # - Dense Transformer needs to be trained for a bit longer to reach low loss value\n",
        "    # - Node positional encodings are not particularly useful\n",
        "    # - Edge distance encodings are very useful\n",
        "    # - Since Cora is highly homophilic, it is important to bias the attention towards nearby nodes\n",
        "\n",
        "def Train_DenseGraphTransformerModel_V2(NUM_LAYERS,\n",
        "              NUM_HEADS,\n",
        "              data):\n",
        "\n",
        "    IN_DIM = data.x.shape[-1]\n",
        "    OUT_DIM = len(data.y.unique())\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    model = DenseGraphTransformerModel_V2(num_layers=NUM_LAYERS, num_heads=NUM_HEADS, in_dim=IN_DIM,out_dim=OUT_DIM).to(device)\n",
        "    data = data.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    def train():\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.pos_enc, data.dense_sp_matrix)\n",
        "        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        return float(loss)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def test():\n",
        "        model.eval()\n",
        "        pred, accs = model(data.x, data.pos_enc, data.dense_sp_matrix).argmax(dim=-1), []\n",
        "        for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "            accs.append(int((pred[mask] == data.y[mask]).sum()) / int(mask.sum()))\n",
        "        return accs\n",
        "\n",
        "    best_val_acc = test_acc = 0\n",
        "    times = []\n",
        "    for epoch in range(1, 100):\n",
        "        start = time.time()\n",
        "        loss = train()\n",
        "        train_acc, val_acc, tmp_test_acc = test()\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            test_acc = tmp_test_acc\n",
        "        # print(f'Epoch: {epoch:04d}, Loss: {loss:.4f} Train: {train_acc:.4f}, '\n",
        "        #       f'Val: {val_acc:.4f}, Test: {tmp_test_acc:.4f}, '\n",
        "        #       f'Final Test: {test_acc:.4f}')\n",
        "        times.append(time.time() - start)\n",
        "    # print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")\n",
        "    return {'train_acc':train_acc,'val_acc':val_acc,'test_acc':test_acc}, model.attn_weights_list\n",
        "\n",
        "    # Notes\n",
        "    # - Dense Transformer needs to be trained for a bit longer to reach low loss value\n",
        "    # - Node positional encodings are not particularly useful\n",
        "    # - Edge distance encodings are very useful\n",
        "    # - Since Cora is highly homophilic, it is important to bias the attention towards nearby nodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veDrDxJIaO7f"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3wGJPl3YJcd"
      },
      "source": [
        "## Training: 1 Layer, 1 Head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6GoLurKW57q"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "NUM_LAYERS = 1\n",
        "NUM_HEADS = 1\n",
        "NUM_RUNS = 10\n",
        "all_stats = {}\n",
        "for data_key in DATASETS:\n",
        "    print(f'Training on {data_key}')\n",
        "    data = DATASETS[data_key]\n",
        "\n",
        "    TRAIN_MASKS = data.train_mask\n",
        "    VAL_MASKS = data.val_mask\n",
        "    TEST_MASKS = data.test_mask\n",
        "\n",
        "    run_stats = {}\n",
        "\n",
        "    for mask_idx in tqdm.tqdm(range(NUM_RUNS)):\n",
        "        data.train_mask = TRAIN_MASKS[:,mask_idx]\n",
        "        data.val_mask = VAL_MASKS[:,mask_idx]\n",
        "        data.test_mask = TEST_MASKS[:,mask_idx]\n",
        "\n",
        "        accuracy_statistics = {}\n",
        "        attn_weights = {}\n",
        "\n",
        "        accuracy_statistics['GCN'], attn_weights['GCN'] = Train_GCN(NUM_LAYERS, NUM_HEADS, data)\n",
        "        accuracy_statistics['SparseGraphTransformerModel'] , attn_weights['SparseGraphTransformerModel'] = Train_SparseGraphTransformerModel(NUM_LAYERS, NUM_HEADS, data)\n",
        "        accuracy_statistics['DenseGraphTransformerModel'] , attn_weights['DenseGraphTransformerModel'] = Train_DenseGraphTransformerModel(NUM_LAYERS, NUM_HEADS, data)\n",
        "        accuracy_statistics['DenseGraphTransformerModel_V2'] , attn_weights['DenseGraphTransformerModel_V2'] = Train_DenseGraphTransformerModel_V2(NUM_LAYERS, NUM_HEADS, data)\n",
        "        attn_weights['SparseGraphTransformerModel'] = torch.stack(attn_weights['SparseGraphTransformerModel']).cpu()\n",
        "        attn_weights['DenseGraphTransformerModel'] = torch.stack(attn_weights['DenseGraphTransformerModel']).cpu()\n",
        "        attn_weights['DenseGraphTransformerModel_V2'] = torch.stack(attn_weights['DenseGraphTransformerModel_V2']).cpu()\n",
        "        run_stats[mask_idx] = {'accuracy': accuracy_statistics, 'attentions': attn_weights}\n",
        "    all_stats[data_key] = run_stats\n",
        "    data.train_mask = TRAIN_MASKS\n",
        "    data.val_mask = VAL_MASKS\n",
        "    data.test_mask = TEST_MASKS\n",
        "\n",
        "import pickle\n",
        "with open('drive/MyDrive/Colab Notebooks/L65_Project/' + f'all_stats_{NUM_LAYERS}L_{NUM_HEADS}H.pkl', 'wb') as f:\n",
        "    pickle.dump(all_stats, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fxj1ye0Y1ZwC"
      },
      "source": [
        "## Training: 1 Layer, 2 Heads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1igyz0Pr1dRN"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "NUM_LAYERS = 1\n",
        "NUM_HEADS = 2\n",
        "NUM_RUNS = 10\n",
        "all_stats = {}\n",
        "for data_key in DATASETS:\n",
        "    print(f'Training on {data_key}')\n",
        "    data = DATASETS[data_key]\n",
        "\n",
        "    TRAIN_MASKS = data.train_mask\n",
        "    VAL_MASKS = data.val_mask\n",
        "    TEST_MASKS = data.test_mask\n",
        "\n",
        "    run_stats = {}\n",
        "\n",
        "    for mask_idx in tqdm.tqdm(range(NUM_RUNS)):\n",
        "        data.train_mask = TRAIN_MASKS[:,mask_idx]\n",
        "        data.val_mask = VAL_MASKS[:,mask_idx]\n",
        "        data.test_mask = TEST_MASKS[:,mask_idx]\n",
        "\n",
        "        accuracy_statistics = {}\n",
        "        attn_weights = {}\n",
        "\n",
        "        accuracy_statistics['GCN'], attn_weights['GCN'] = Train_GCN(NUM_LAYERS, NUM_HEADS, data)\n",
        "        accuracy_statistics['SparseGraphTransformerModel'] , attn_weights['SparseGraphTransformerModel'] = Train_SparseGraphTransformerModel(NUM_LAYERS, NUM_HEADS, data)\n",
        "        accuracy_statistics['DenseGraphTransformerModel'] , attn_weights['DenseGraphTransformerModel'] = Train_DenseGraphTransformerModel(NUM_LAYERS, NUM_HEADS, data)\n",
        "        accuracy_statistics['DenseGraphTransformerModel_V2'] , attn_weights['DenseGraphTransformerModel_V2'] = Train_DenseGraphTransformerModel_V2(NUM_LAYERS, NUM_HEADS, data)\n",
        "        attn_weights['SparseGraphTransformerModel'] = torch.stack(attn_weights['SparseGraphTransformerModel']).cpu()\n",
        "        attn_weights['DenseGraphTransformerModel'] = torch.stack(attn_weights['DenseGraphTransformerModel']).cpu()\n",
        "        attn_weights['DenseGraphTransformerModel_V2'] = torch.stack(attn_weights['DenseGraphTransformerModel_V2']).cpu()\n",
        "        run_stats[mask_idx] = {'accuracy': accuracy_statistics, 'attentions': attn_weights}\n",
        "    all_stats[data_key] = run_stats\n",
        "    data.train_mask = TRAIN_MASKS\n",
        "    data.val_mask = VAL_MASKS\n",
        "    data.test_mask = TEST_MASKS\n",
        "\n",
        "import pickle\n",
        "with open('drive/MyDrive/Colab Notebooks/L65_Project/' + f'all_stats_{NUM_LAYERS}L_{NUM_HEADS}H.pkl', 'wb') as f:\n",
        "    pickle.dump(all_stats, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWeKuQTt1k_M"
      },
      "source": [
        "## Training: 2 Layers, 1 Head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcAfw7My1oij"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "### Train Cora ###\n",
        "NUM_LAYERS = 2\n",
        "NUM_HEADS = 1\n",
        "NUM_RUNS = 10\n",
        "# data_key = 'Wisconsin'\n",
        "all_stats = {}\n",
        "for data_key in DATASETS:\n",
        "    print(f'Training on {data_key}')\n",
        "    data = DATASETS[data_key]\n",
        "\n",
        "    TRAIN_MASKS = data.train_mask\n",
        "    VAL_MASKS = data.val_mask\n",
        "    TEST_MASKS = data.test_mask\n",
        "\n",
        "    run_stats = {}\n",
        "\n",
        "    for mask_idx in tqdm.tqdm(range(NUM_RUNS)):\n",
        "        data.train_mask = TRAIN_MASKS[:,mask_idx]\n",
        "        data.val_mask = VAL_MASKS[:,mask_idx]\n",
        "        data.test_mask = TEST_MASKS[:,mask_idx]\n",
        "\n",
        "        accuracy_statistics = {}\n",
        "        attn_weights = {}\n",
        "\n",
        "        accuracy_statistics['GCN'], attn_weights['GCN'] = Train_GCN(NUM_LAYERS, NUM_HEADS, data)\n",
        "        accuracy_statistics['SparseGraphTransformerModel'] , attn_weights['SparseGraphTransformerModel'] = Train_SparseGraphTransformerModel(NUM_LAYERS, NUM_HEADS, data)\n",
        "        accuracy_statistics['DenseGraphTransformerModel'] , attn_weights['DenseGraphTransformerModel'] = Train_DenseGraphTransformerModel(NUM_LAYERS, NUM_HEADS, data)\n",
        "        accuracy_statistics['DenseGraphTransformerModel_V2'] , attn_weights['DenseGraphTransformerModel_V2'] = Train_DenseGraphTransformerModel_V2(NUM_LAYERS, NUM_HEADS, data)\n",
        "        attn_weights['SparseGraphTransformerModel'] = torch.stack(attn_weights['SparseGraphTransformerModel'])\n",
        "        attn_weights['DenseGraphTransformerModel'] = torch.stack(attn_weights['DenseGraphTransformerModel'])\n",
        "        attn_weights['DenseGraphTransformerModel_V2'] = torch.stack(attn_weights['DenseGraphTransformerModel_V2'])\n",
        "        run_stats[mask_idx] = {'accuracy': accuracy_statistics, 'attentions': attn_weights}\n",
        "    all_stats[data_key] = run_stats\n",
        "    data.train_mask = TRAIN_MASKS\n",
        "    data.val_mask = VAL_MASKS\n",
        "    data.test_mask = TEST_MASKS\n",
        "\n",
        "import pickle\n",
        "with open('drive/MyDrive/Colab Notebooks/L65_Project/' + f'all_stats_{NUM_LAYERS}L_{NUM_HEADS}H.pkl', 'wb') as f:\n",
        "    pickle.dump(all_stats, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hMUb2yK1pEB"
      },
      "source": [
        "## Training: 2 Layers, 2 Heads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYmZVaET1sKD"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import tqdm\n",
        "### Train Cora ###\n",
        "NUM_LAYERS = 2\n",
        "NUM_HEADS = 2\n",
        "NUM_RUNS = 10\n",
        "# data_key = 'Wisconsin'\n",
        "all_stats = {}\n",
        "for data_key in DATASETS:\n",
        "    print(f'Training on {data_key}')\n",
        "    data = DATASETS[data_key]\n",
        "\n",
        "    TRAIN_MASKS = data.train_mask\n",
        "    VAL_MASKS = data.val_mask\n",
        "    TEST_MASKS = data.test_mask\n",
        "\n",
        "    run_stats = {}\n",
        "\n",
        "    for mask_idx in tqdm.tqdm(range(NUM_RUNS)):\n",
        "        gc.collect()\n",
        "        data.train_mask = TRAIN_MASKS[:,mask_idx]\n",
        "        data.val_mask = VAL_MASKS[:,mask_idx]\n",
        "        data.test_mask = TEST_MASKS[:,mask_idx]\n",
        "\n",
        "        accuracy_statistics = {}\n",
        "        attn_weights = {}\n",
        "\n",
        "        accuracy_statistics['GCN'], attn_weights['GCN'] = Train_GCN(NUM_LAYERS, NUM_HEADS, data)\n",
        "        accuracy_statistics['SparseGraphTransformerModel'] , attn_weights['SparseGraphTransformerModel'] = Train_SparseGraphTransformerModel(NUM_LAYERS, NUM_HEADS, data)\n",
        "        accuracy_statistics['DenseGraphTransformerModel'] , attn_weights['DenseGraphTransformerModel'] = Train_DenseGraphTransformerModel(NUM_LAYERS, NUM_HEADS, data)\n",
        "        accuracy_statistics['DenseGraphTransformerModel_V2'] , attn_weights['DenseGraphTransformerModel_V2'] = Train_DenseGraphTransformerModel_V2(NUM_LAYERS, NUM_HEADS, data)\n",
        "        attn_weights['SparseGraphTransformerModel'] = torch.stack(attn_weights['SparseGraphTransformerModel']).cpu()\n",
        "        attn_weights['DenseGraphTransformerModel'] = torch.stack(attn_weights['DenseGraphTransformerModel']).cpu()\n",
        "        attn_weights['DenseGraphTransformerModel_V2'] = torch.stack(attn_weights['DenseGraphTransformerModel_V2']).cpu()\n",
        "        run_stats[mask_idx] = {'accuracy': accuracy_statistics, 'attentions': attn_weights}\n",
        "\n",
        "    all_stats[data_key] = run_stats\n",
        "    data.train_mask = TRAIN_MASKS\n",
        "    data.val_mask = VAL_MASKS\n",
        "    data.test_mask = TEST_MASKS\n",
        "\n",
        "import pickle\n",
        "with open('drive/MyDrive/Colab Notebooks/L65_Project/' + f'all_stats_{NUM_LAYERS}L_{NUM_HEADS}H.pkl', 'wb') as f:\n",
        "    pickle.dump(all_stats, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXuPgGbGa2Nd"
      },
      "source": [
        "# Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "670dtom5sl10"
      },
      "source": [
        "## Table 2: Accuracy Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNkbWu7zH06U"
      },
      "outputs": [],
      "source": [
        "### Table 2 ###\n",
        "### Accuracy Statistics ###\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "all_stats_df = {}\n",
        "for data_key in all_stats:\n",
        "  run_stats = all_stats[data_key]\n",
        "  table1 = pd.concat({key : pd.DataFrame(run_stats[key]['accuracy']) for key in run_stats}, axis=0)\n",
        "  table1_train = pd.concat({'mean': table1.mean(level=1, axis=0).loc['train_acc'], 'std':table1.std(level=1).loc['train_acc']}, axis=1)\n",
        "  table1_test = pd.concat({'mean': table1.mean(level=1, axis=0).loc['test_acc'], 'std':table1.std(level=1).loc['test_acc']}, axis=1)\n",
        "  # table1 = pd.concat({'Train': table1_train, 'Test': table1_test}, axis=1)\n",
        "  table1 = table1_test\n",
        "  all_stats_df[data_key] = table1\n",
        "pd.concat(all_stats_df, axis=1).round(2)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Fxj1ye0Y1ZwC",
        "sWeKuQTt1k_M",
        "2hMUb2yK1pEB",
        "ZXuPgGbGa2Nd",
        "670dtom5sl10"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}